---
title: "Homework 4"
author: "Joseph Lynch and Anand Taxali"
date: "6/6/2020"
output: html_document
---

```{r}

####Problem 1######
library(readxl)
UniversalBank <- read_excel("C:/Users/Administrator/Documents/UniversalBank.xlsx")
nrow(UniversalBank)

#Partition the data into training 60% and validation 40% sets:
UniversalBank60 <- UniversalBank[1:3000,]
UniversalBank40 <- UniversalBank[3001:5000,]

#1a

library(class)
defaultclass <- UniversalBank60$`Personal Loan`
defaulttest <- UniversalBank40$`Personal Loan`
training<-model.matrix(~Age+Experience+Income+Family+CCAvg+factor(Education)-
                               1+Mortgage+`Securities Account`+`CD Account`+Online+CreditCard,data=UniversalBank60)
test<-model.matrix(~Age+Experience+Income+Family+CCAvg+factor(Education)-
                           1+Mortgage+`Securities Account`+`CD Account`+Online+CreditCard,data=UniversalBank40)
knn(training,c(40,10,84,2,2,0,1,0,0,0,0,1,1),defaultclass,k=1)

#This customer would be classified as zero



#1b What is a choice of k that balances between overfitting and ignoring the predictor information?

knn.1 <- knn(training, test, defaultclass, k=1)
knn.2 <- knn(training, test, defaultclass, k=2)
knn.3 <- knn(training, test, defaultclass, k=3)
knn.4 <- knn(training, test, defaultclass, k=4)
knn.5 <- knn(training, test, defaultclass, k=5)
knn.6 <- knn(training, test, defaultclass, k=6)
knn.7 <- knn(training, test, defaultclass, k=7)
knn.8 <- knn(training, test, defaultclass, k=8)

table(knn.1 ,defaulttest)
table(knn.2 ,defaulttest)
table(knn.3 ,defaulttest)
table(knn.4 ,defaulttest)
table(knn.5 ,defaulttest)
table(knn.6 ,defaulttest)
table(knn.7 ,defaulttest)
table(knn.8 ,defaulttest)

#k=7 is a good balance between overfitting and ignoring the predictor information

#1c. Show the classification matrix for the validation data that results from using the bestk.

table(knn.7 ,defaulttest)

#1d
knn(training,c(40,10,84,2,2,0,1,0,0,0,0,1,1),defaultclass,k=7)
#This customer belongs to class 0


#1e Repartition the data, this time into training, validation, and test sets (50% : 30% :
#20%). Apply the k-NN method with the k chosen above. Compare the classification
#matrix of the test set with that of the training and validation sets. Comment on the
#differences and their reason

UniversalBank50 <- UniversalBank[1:2500,]
UniversalBank30 <- UniversalBank[2501:4000,]
UniversalBank20 <- UniversalBank[4001:5000,]
training<-model.matrix(~Age+Experience+Income+Family+CCAvg+factor(Education)-
                               1+Mortgage+`Securities Account`+`CD Account`+Online+CreditCard,data=UniversalBank50)
validation <-model.matrix(~Age+Experience+Income+Family+CCAvg+factor(Education)-
                                  1+Mortgage+`Securities Account`+`CD Account`+Online+CreditCard,data=UniversalBank30)
test<-model.matrix(~Age+Experience+Income+Family+CCAvg+factor(Education)-
                           1+Mortgage+`Securities Account`+`CD Account`+Online+CreditCard,data=UniversalBank20)
defaulttraining <- UniversalBank50$`Personal Loan`
defaultvalidation <- UniversalBank30$`Personal Loan`
defaulttest <- UniversalBank20$`Personal Loan`


#training and validation set classification matrix
knn.tv<-knn(training,validation,defaulttraining,k=7)

table(knn.tv ,defaultvalidation)

#training and test set classification matrix
knn.tt<-knn(training,test,defaulttraining,k=7)

table(knn.tt ,defaulttest)

#training set classification matrix
knn.ttraining<-knn(training,training,defaulttraining,k=7)

table(knn.ttraining ,defaulttraining)

#Since the model was trained by the training data, we can see this is why the training classification matrix has the best performance
#The validation and test classification matrices have similar accuracy

```{r}


####Problem 2######

library(readxl)
BostonHousing <- read_excel("C:/Users/Administrator/Documents/BostonHousing.xlsx")
nrow(BostonHousing)

normalizedData = as.data.frame(scale(BostonHousing[,-c(13,14)])) # For Normalizing Data
trainData <- normalizedData[1:303, ]
validData <- normalizedData[304:506, ]
defaulttrain <- BostonHousing$`CAT. MEDV`[1:303]
defaultvalid <- BostonHousing$`CAT. MEDV`[304:506]
 
BostonHousing <-BostonHousing[,-c(13,14)]
 
library(class)
library(kknn)

knn.1 = knn(trainData, validData, cl = defaulttrain , k= 1, prob = TRUE)
knn.2 = knn(trainData, validData, cl = defaulttrain , k= 2, prob = TRUE)
knn.3 = knn(trainData, validData, cl = defaulttrain , k= 3, prob = TRUE)
knn.4 = knn(trainData, validData, cl = defaulttrain , k= 4, prob = TRUE)
knn.5 = knn(trainData, validData, cl = defaulttrain , k= 5, prob = TRUE)

 
table(knn.1, defaultvalid)
table(knn.2, defaultvalid)
table(knn.3, defaultvalid)
table(knn.4, defaultvalid)
table(knn.5, defaultvalid)

#2b Predict the MEDV for a tract with the following information, using the best k:

newcase = c(0.2, 0, 7, 0, 0.538, 6, 62, 4.7, 4, 307, 21, 10)
for (i in c(1:12)){
        
        std_dev <- unlist(BostonHousing[,1])
        std_dev<-  sd(as.numeric(std_dev))
newcase[i] = (newcase[i] - 
                      
                      ifelse(std_dev==0,newcase[i]-newcase[i], (sapply(BostonHousing[,i], mean, na.rm = TRUE)/std_dev))
              
              )
 }

class::knn(trainData, t(newcase), cl = defaulttrain , k= 5, prob = TRUE)

#The new case is classified as 0, which means  the value of MEDV is less than 30.

#2c Why is the error of the training data zero?
#The nearest neighbors in the training set will be the data points themselves, so by calculating by the Euclidean distance
#every class will be calculated exactly ie 0, which overfits the data to give an error of zero

#2d. Why is the validation data error overly optimistic compared to the error rate when
#applying this k-NN predictor to new data?

# The model that is chosen from the validation set does best on validation data hence the overly optimistic error rate, but it will perform worse against new data. 


#2e If the purpose is to predict MEDV for several thousands of new tracts, what would
#be the disadvantage of using k-NN prediction? List the operations that the algorithm
#goes through in order to produce each prediction.

#The disadvantage is that the k-NN prediction algorithm is a "lazy learner", so the process would be far too time consuming.
#Each individual variable would need to normalized, calculate each case's distance, sort by distance and take a weighted average response value as the prediction



```{r}

#####Problem 3######

library(readxl)
Accidents <- read_excel("C:/Users/Administrator/Documents/Accidents.xlsx")
nrow(Accidents)

Accidents$INJURY <- ifelse(Accidents$MAX_SEV_IR == 0, 0, 1)
Accidents$INJURY <- factor(Accidents$INJURY, levels = c(0, 1), labels = c("No", "Yes"))

#Using the information in this dataset, if an accident has just been reported and no
#further information is available, what should the prediction be? (INJURY = Yes or No?) Why?
table(Accidents$INJURY)

#We should report an accident based on the naive rule since the majority of accidents have an injury reported

#Select the first 12 records in the dataset and look only at the response (INJURY) and
#the two predictors WEATHER_R and TRAF_CON_R

#i.
subset <- Accidents[1:12, c(16,19,25)]
subset$TRAF_CON_R <- factor(subset$TRAF_CON_R)
subset$WEATHER_R <- factor(subset$WEATHER_R)
subset$INJURY <- factor(subset$INJURY)

table(subset$WEATHER_R, subset$TRAF_CON_R, subset$INJURY, dnn = c("WEATHER_R","TRAF_CON_R", "INJURY"))

unique(subset$TRAF_CON_R)
unique(subset$WEATHER_R)

#ii. Compute the exact Bayes conditional probabilities of an injury (INJURY =Yes) given the six possible combinations of the predictors

subset[subset$TRAF_CON_R ==0 & subset$WEATHER_R==1,]   # is (2/3)
subset[subset$TRAF_CON_R ==1 & subset$WEATHER_R==1,]   # is (0)
subset[subset$TRAF_CON_R ==2 & subset$WEATHER_R==1,]   # is (0)

subset[subset$TRAF_CON_R ==0 & subset$WEATHER_R==2,]  # is  (1/6)
subset[subset$TRAF_CON_R ==1 & subset$WEATHER_R==2,]  # is  (0)   
subset[subset$TRAF_CON_R ==2 & subset$WEATHER_R==2,]  # is (0)

```{r}

#iii.
#Given the 12 instances in the sample, the only instances that would be classified with the .5 cutoff 
#as injuries would be with TRAF_CON_R=0 and Weather_R=1

data.frame(colnames(Accidents)) 

#iv. Compute manually the naive Bayes conditional probability of an injury given WEATHER_R = 1 and TRAF_CON_R = 1

subset[subset$INJURY=="Yes" & subset$WEATHER_R==1,]
subset[subset$INJURY=="Yes" & subset$TRAF_CON_R ==1,]
subset[subset$INJURY=="Yes",]

subset[subset$INJURY=="No" & subset$WEATHER_R==1,]
subset[subset$INJURY=="No" & subset$TRAF_CON_R ==1,]
subset[subset$INJURY=="No",]

#Manual calculation = P(Injury | WEATHER_R = 1 and TRAF_CON_R = 1) = ((3/12)*(2/3)*(0/3)) / [((3/12)*(2/3)*(0/3)) +((9/12)*(3/9)*(2/9))] = 0

#v. Run a naive Bayes classifier on the 12 records and 2 predictors. Obtain
#probabilities and classifications for all 12 records. Compare this to the exact
#Bayes classification. Are the resulting classifications equivalent? Is the
#ranking (= ordering) of observations equivalent?

library(e1071)

NaiveSubset <- naiveBayes(INJURY ~ TRAF_CON_R + WEATHER_R, subset)
NaiveSubset

Prediction <- predict(NaiveSubset, newdata = subset, type = "raw")
Prediction

ClassPrediction <- c("No", "Yes", "No", "No", "No", "No", "No", "No", "No", "Yes", "No", "Yes")
Results <- data.frame(predicted = ClassPrediction,actual = subset$INJURY ,Prediction)
Results

#The resulting classifications and rankings are equivalent



#3c. Let us now return to the entire dataset. Partition the data into training/validation sets
#Assuming that no information or initial reports about the accident itself are available at the time of 
#prediction (only location characteristics, weatherconditions, etc.), 
#which predictors can we include in the analysis? (Use the Data_Codes sheet.)

set.seed(1)
train.index <- sample(nrow(Accidents), 0.6*nrow(Accidents))
train.df <- Accidents[train.index, ]
valid.df <- Accidents[-train.index, ]

#We'll include every predictor in our analysis, but remove the extra reponse variable (MAx_SEV_IR)

Accidents <- Accidents[, -24]

Accidents$HOUR_I_R <- as.factor(Accidents$HOUR_I_R)
Accidents$ALCHL_I <- as.factor(Accidents$ALCHL_I)
Accidents$ALIGN_I <- as.factor(Accidents$ALIGN_I)
Accidents$STRATUM_R <- as.factor(Accidents$STRATUM_R)
Accidents$WRK_ZONE <- as.factor(Accidents$WRK_ZONE)
Accidents$WKDY_I_R <- as.factor(Accidents$WKDY_I_R)
Accidents$INT_HWY <- as.factor(Accidents$INT_HWY)
Accidents$LGTCON_I_R <- as.factor(Accidents$LGTCON_I_R)
Accidents$MANCOL_I_R <- as.factor(Accidents$MANCOL_I_R)
Accidents$PED_ACC_R <- as.factor(Accidents$PED_ACC_R)
Accidents$RELJCT_I_R <- as.factor(Accidents$RELJCT_I_R)
Accidents$REL_RWY_R <- as.factor(Accidents$REL_RWY_R)
Accidents$PROFIL_I_R <- as.factor(Accidents$PROFIL_I_R)
Accidents$SPD_LIM <- as.factor(Accidents$SPD_LIM)
Accidents$SUR_COND <- as.factor(Accidents$SUR_COND)
Accidents$TRAF_CON_R <- as.factor(Accidents$TRAF_CON_R)
Accidents$TRAF_WAY <- as.factor(Accidents$TRAF_WAY)
Accidents$VEH_INVL <- as.factor(Accidents$VEH_INVL)
Accidents$WEATHER_R <- as.factor(Accidents$WEATHER_R)
Accidents$INJURY_CRASH <- as.factor(Accidents$INJURY_CRASH)
Accidents$NO_INJ_I <- as.factor(Accidents$NO_INJ_I)
Accidents$PRPTYDMG_CRASH <- as.factor(Accidents$PRPTYDMG_CRASH)
Accidents$FATALITIES <- as.factor(Accidents$FATALITIES)
Accidents$INJURY <- as.factor(Accidents$INJURY)

#ii. Run a naive Bayes classifier on the complete training set with the relevant
#predictors (and INJURY as the response). Note that all predictors are
#categorical. Show the classification matrix.
library(caret)
Training <- naiveBayes(INJURY ~ ., train.df)
Training
pred.prob <- predict(Training, newdata = train.df, type = "raw")
pred.class <- predict(Training, newdata = train.df)
confusionMatrix(pred.class, train.df$INJURY)

#iii. What is the overall error for the validation set? 
pred.prob.valid <- predict(Training, newdata = valid.df, type = "raw")
pred.class.valid <- predict(Training, newdata = valid.df)
confusionMatrix(pred.class.valid,valid.df$INJURY)

#The overall error for the validaton set is zero.

#iv. What is the percent improvement relative to the naive rule (using the validation set)?
# Both error rates are zero(test and validation)

#v. Examine the conditional probabilities output. Why do we get a probability of
#zero for P(INJURY =No | SPD_LIM = 5)?
table(valid.df$SPD_LIM,valid.df$INJURY)
#There only exists one case with no injury with a speed limit of 5, which is why we get a probability of zero 



